{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOi95Ytf7mXuW4i4UXds/O3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohan-damagatla/LetsUpgradeDataScience/blob/main/Datakathon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "of2bm7FGnNRp"
      },
      "source": [
        "!pip install catboost #Install catboost on colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1ytz2OBnPUX"
      },
      "source": [
        "#Reading input data and Onehotencoding the categorical columns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "df = pd.read_excel('/content/train.xlsx')\n",
        "\n",
        "primarycol = 'JOBID'\n",
        "depcol = 'ELAPSEDTIME'\n",
        "indcol = list(df.columns)\n",
        "indcol.remove(primarycol)\n",
        "indcol.remove(depcol)\n",
        "    \n",
        "independentcols=list(df.columns)\n",
        "removecols=[depcol,primarycol]\n",
        "independentcols = [n for n in independentcols if n not in removecols]\n",
        "\n",
        "encoded={}\n",
        "cat_col=[]\n",
        "for col, col_type in df.dtypes.iteritems():\n",
        "      if col_type == 'O':\n",
        "          cat_col.append(col)\n",
        "cat_col = [x for x in cat_col if x in indcol]\n",
        "for i in range(len(cat_col)):\n",
        "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "    enc_df = pd.DataFrame(encoder.fit_transform(df[[cat_col[i]]]).toarray())\n",
        "    enc_df.columns = encoder.get_feature_names([cat_col[i]])\n",
        "    df=df.drop(cat_col[i], axis=1)\n",
        "    df=pd.concat([df, enc_df], axis=1)\n",
        "    encoded.update({cat_col[i]:encoder})\n",
        "    \n",
        "independentcols=list(df.columns)\n",
        "removecols=[depcol,primarycol]\n",
        "independentcols = [n for n in independentcols if n not in removecols]\n",
        "\n",
        "x=df[independentcols].values\n",
        "y=df[depcol].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keFYZvnynjEe"
      },
      "source": [
        "#Applying log tranformation on the output column\n",
        "from scipy import stats\n",
        "from scipy.stats import norm, skew\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.distplot(y , fit=norm);\n",
        "\n",
        "# Get the fitted parameters used by the function\n",
        "(mu, sigma) = norm.fit(y)\n",
        "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
        "\n",
        "#Now plot the distribution\n",
        "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('SalePrice distribution')\n",
        "\n",
        "#Get also the QQ-plot\n",
        "fig = plt.figure()\n",
        "res = stats.probplot(y, plot=plt)\n",
        "plt.show()\n",
        "\n",
        "y = np.log(y)\n",
        "\n",
        "#Check the new distribution \n",
        "sns.distplot(y , fit=norm);\n",
        "\n",
        "# Get the fitted parameters used by the function\n",
        "(mu, sigma) = norm.fit(y)\n",
        "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
        "\n",
        "#Now plot the distribution\n",
        "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
        "            loc='best')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('SalePrice distribution')\n",
        "\n",
        "#Get also the QQ-plot\n",
        "fig = plt.figure()\n",
        "res = stats.probplot(y, plot=plt)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b--CpYVsnzH0"
      },
      "source": [
        "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
        "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
        "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "#Define the cross validation score\n",
        "n_folds = 5\n",
        "\n",
        "def rmsle_cv(model):\n",
        "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(x)\n",
        "    rmse= np.sqrt(-cross_val_score(model, x, y, scoring=\"neg_mean_squared_error\", cv = kf))\n",
        "    return(rmse)\n",
        "\n",
        "#Define gradient boost with best obtained params\n",
        "GBoost = GradientBoostingRegressor(n_estimators=5000, learning_rate=0.2,\n",
        "                                   max_depth=10, max_features='sqrt',\n",
        "                                   min_samples_leaf=15, min_samples_split=10, \n",
        "                                   loss='huber', random_state =5)\n",
        "\n",
        "#Define XGboost with best params\n",
        "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
        "                             learning_rate=0.15, max_depth=5, \n",
        "                             min_child_weight=1.7817, n_estimators=2000,\n",
        "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
        "                             subsample=0.5213, silent=1,\n",
        "                             random_state =7, nthread = -1)\n",
        "\n",
        "#Define Lightgbm with best params\n",
        "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,max_depth=10,\n",
        "                              learning_rate=0.3, n_estimators=5000,\n",
        "                              max_bin = 55, bagging_fraction = 0.9,\n",
        "                              bagging_freq = 5, feature_fraction = 0.9,\n",
        "                              feature_fraction_seed=9, bagging_seed=9,\n",
        "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
        "\n",
        "#Define Lightgbm with best params\n",
        "model_cat = CatBoostRegressor(verbose=0,depth= 10, l2_leaf_reg = 1, iterations = 5000, learning_rate = 0.1)\n",
        "\n",
        "#Form a stacking average model using the above models as base and meta models\n",
        "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
        "    def __init__(self, base_models, meta_model, n_folds=5):\n",
        "        self.base_models = base_models\n",
        "        self.meta_model = meta_model\n",
        "        self.n_folds = n_folds\n",
        "   \n",
        "    # We again fit the data on clones of the original models\n",
        "    def fit(self, X, y):\n",
        "        self.base_models_ = [list() for x in self.base_models]\n",
        "        self.meta_model_ = clone(self.meta_model)\n",
        "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
        "        \n",
        "        # Train cloned base models then create out-of-fold predictions\n",
        "        # that are needed to train the cloned meta-model\n",
        "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
        "        for i, model in enumerate(self.base_models):\n",
        "            for train_index, holdout_index in kfold.split(X, y):\n",
        "                instance = clone(model)\n",
        "                self.base_models_[i].append(instance)\n",
        "                instance.fit(X[train_index], y[train_index])\n",
        "                y_pred = instance.predict(X[holdout_index])\n",
        "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
        "                \n",
        "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
        "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
        "        return self\n",
        "   \n",
        "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
        "    #meta-features for the final prediction which is done by the meta-model\n",
        "    def predict(self, X):\n",
        "        meta_features = np.column_stack([\n",
        "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
        "            for base_models in self.base_models_ ])\n",
        "        return self.meta_model_.predict(meta_features)\n",
        "\n",
        "stacked_averaged_models = StackingAveragedModels(base_models = (GBoost, model_xgb,model_cat),\n",
        "                                                 meta_model = model_lgb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aznQRgXUpAQ0"
      },
      "source": [
        "#Read test data and apply the same encoder for categorical columns as train data\n",
        "test = pd.read_excel('/content/test1.xlsx')\n",
        "\n",
        "for i in range(len(cat_col)):\n",
        "\tenc_df = pd.DataFrame(encoded[cat_col[i]].transform(test[[cat_col[i]]]).toarray())\n",
        "\tenc_df.columns = encoded[cat_col[i]].get_feature_names([cat_col[i]])\n",
        "\ttest=test.drop(cat_col[i], axis=1)\n",
        "\ttest=pd.concat([test, enc_df], axis=1)\n",
        "test_x=test[independentcols].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmXSjJwOpQQj"
      },
      "source": [
        "def rmsle(y, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y, y_pred))\n",
        "\n",
        "#Modelling using train data and predicting on test data\n",
        "stacked_averaged_models.fit(x, y)\n",
        "stacked_train_pred = stacked_averaged_models.predict(x)\n",
        "stacked_pred = np.exp(stacked_averaged_models.predict(test_x))\n",
        "print(rmsle(y, stacked_train_pred))\n",
        "\n",
        "model_cat.fit(x, y)\n",
        "cat_train_pred = model_cat.predict(x)\n",
        "cat_pred = np.exp(model_cat.predict(test_x))\n",
        "print(rmsle(y, cat_train_pred))\n",
        "\n",
        "\n",
        "GBoost.fit(x, y)\n",
        "grad_train_pred = GBoost.predict(x)\n",
        "grad_pred = np.exp(GBoost.predict(test_x))\n",
        "print(rmsle(y, grad_train_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyTwGwNipdsI"
      },
      "source": [
        "#Weighted values for predicting the output\n",
        "ensemble = cat_pred*0.49 + stacked_pred*0.5 + grad_pred*0.05\n",
        "test['PREDICTION'] = ensemble\n",
        "\n",
        "test = test[[primarycol,'PREDICTION']]\n",
        "test.to_csv('data_cbgridcvgb.csv', sep=';', index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download('data_cbgridcvgb.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}